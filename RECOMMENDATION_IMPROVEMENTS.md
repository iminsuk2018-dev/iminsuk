# ì¶”ì²œ ì‹œìŠ¤í…œ ê°œì„  ì‚¬í•­

## í˜„ì¬ êµ¬í˜„ ìƒíƒœ

### âœ… ì™„ì„±ëœ ê¸°ëŠ¥

1. **TF-IDF ë²¡í„°í™” ê¸°ë°˜ ì¶”ì²œ**
   - `vectorizer.py`: sklearn TF-IDF ê¸°ë°˜ ë¬¸ì„œ ìœ ì‚¬ë„ ê³„ì‚°
   - ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± (ë¬¸ì„œ ì œëª©, ì´ˆë¡, íƒœê·¸, ì£¼ì„ í¬í•¨)
   - Cosine similarityë¥¼ í†µí•œ ì¶”ì²œ ìŠ¤ì½”ì–´ë§

2. **Crossref API í†µí•©**
   - `journal_fetcher.py`: í•™ìˆ  ì €ë„ ìµœê·¼ ë…¼ë¬¸ ìˆ˜ì§‘
   - ì €ë„ëª…, ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§
   - DOI, ISSN ê²€ìƒ‰ ê¸°ëŠ¥

3. **ì¶”ì²œ ì—”ì§„**
   - `engine.py`: í†µí•© ì¶”ì²œ ë¡œì§
   - ì‚¬ìš©ì ì½”í¼ìŠ¤ ìë™ ìƒì„±
   - ì¶”ì²œ ì´ìœ  ì„¤ëª… (ê³µí†µ í‚¤ì›Œë“œ, ìœ ì‚¬ ë¬¸ì„œ)

4. **ì¶”ì²œ UI**
   - `recommendation_dialog.py`: ì¶”ì²œ ë‹¤ì´ì–¼ë¡œê·¸
   - ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì²˜ë¦¬
   - ì§„í–‰ìƒí™© í‘œì‹œ

## ğŸ”´ ë¯¸ì™„ì„±/ë¬¸ì œì 

### 1. arXiv API í†µí•© ë¯¸êµ¬í˜„
**íŒŒì¼:** `core/recommendation/journal_fetcher.py:184-231`

```python
# TODO: Implement XML parsing
logger.warning("arXiv parsing not implemented yet")
return []
```

**ë¬¸ì œ:**
- ArxivFetcher í´ë˜ìŠ¤ëŠ” ìˆì§€ë§Œ XML íŒŒì‹±ì´ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
- arXivëŠ” ë¬´ë£Œ í”„ë¦¬í”„ë¦°íŠ¸ ì €ì¥ì†Œë¡œ ë§¤ìš° ìœ ìš©í•¨

**í•´ê²° ë°©ë²•:**
```python
import xml.etree.ElementTree as ET

def fetch_recent_articles(self, category: str, max_results: int = 50):
    # ... API í˜¸ì¶œ ...

    # Parse XML
    root = ET.fromstring(response.content)
    namespace = {'atom': 'http://www.w3.org/2005/Atom'}

    articles = []
    for entry in root.findall('atom:entry', namespace):
        title = entry.find('atom:title', namespace).text
        abstract = entry.find('atom:summary', namespace).text
        # ... íŒŒì‹± ë¡œì§ ...
```

### 2. ì €ë„ ì¦ê²¨ì°¾ê¸° ê´€ë¦¬ ë¯¸ì™„ì„±
**DB í…Œì´ë¸”:** `favorite_journals` (ì¡´ì¬)

**í˜„ì¬ ìƒíƒœ:**
- DBì— í…Œì´ë¸”ì€ ìˆì§€ë§Œ ë§¤ë‹ˆì € í´ë˜ìŠ¤ê°€ ì—†ìŒ
- UIì—ì„œ ì¦ê²¨ì°¾ê¸° ê´€ë¦¬ ë¶ˆê°€ëŠ¥

**í•„ìš” ê¸°ëŠ¥:**
- ì €ë„ ì¶”ê°€/ì œê±° (`FavoriteJournalManager`)
- ì •ê¸° ìë™ ìŠ¤ìº” (ë§¤ì£¼/ë§¤ì¼)
- ìƒˆ ë…¼ë¬¸ ì•Œë¦¼

### 3. ì¶”ì²œ ìºì‹± ë¯¸êµ¬í˜„
**DB í…Œì´ë¸”:** `recommendation_cache` (ì¡´ì¬)

**ë¬¸ì œ:**
- Crossref APIëŠ” rate limit ìˆìŒ
- ë§¤ë²ˆ API í˜¸ì¶œí•˜ë©´ ëŠë¦¼
- ìºì‹œ í…Œì´ë¸”ì´ ìˆì§€ë§Œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

**í•´ê²°:**
```python
def generate_recommendations(self, journal_name, days_back):
    # Check cache first
    cached = self._get_cached_recommendations(journal_name, max_age_hours=24)
    if cached:
        return cached

    # Generate new recommendations
    recommendations = ...

    # Save to cache
    self._save_to_cache(journal_name, recommendations)

    return recommendations
```

### 4. ì¶”ì²œ ì„¤ëª… ê°œì„  í•„ìš”
**íŒŒì¼:** `core/recommendation/engine.py:128-164`

**í˜„ì¬ ë¬¸ì œ:**
- ì„¤ëª…ì´ ë‹¨ìˆœí•¨ ("ë†’ì€ ê´€ë ¨ì„± (ìœ ì‚¬ë„: 0.75)")
- ì™œ ì¶”ì²œë˜ëŠ”ì§€ ëª…í™•í•˜ì§€ ì•ŠìŒ

**ê°œì„ :**
- ë” êµ¬ì²´ì ì¸ ì„¤ëª…
- ì‹œê°ì  í‘œí˜„ (ì§„í–‰ ë°”, ì•„ì´ì½˜)
- ì¶”ì²œ ê·¼ê±°ë¥¼ ì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ ì œì‹œ
  - ë‚´ìš© ìœ ì‚¬ë„
  - ì €ì ê²¹ì¹¨
  - ì¸ìš© ê´€ê³„
  - íƒœê·¸ ìœ ì‚¬ì„±

### 5. ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ì—†ìŒ
**ë¬¸ì œ:**
- ì¶”ì²œì´ ìœ ìš©í•œì§€ í”¼ë“œë°± ìˆ˜ì§‘ ì•ˆ í•¨
- ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ ê°œì„  ë¶ˆê°€ëŠ¥

**í•„ìš” ê¸°ëŠ¥:**
- ì¶”ì²œ í•­ëª©ì— ğŸ‘/ğŸ‘ ë²„íŠ¼
- í”¼ë“œë°± DB ì €ì¥
- í”¼ë“œë°± ê¸°ë°˜ ì¬í•™ìŠµ

### 6. ë‹¤ì–‘í•œ ì¶”ì²œ ì†ŒìŠ¤ ë¶€ì¡±
**í˜„ì¬:** Crossrefë§Œ ì§€ì›

**ê°œì„  ë°©í–¥:**
- arXiv (í”„ë¦¬í”„ë¦°íŠ¸)
- PubMed (ìƒëª…ê³¼í•™)
- IEEE Xplore (ê³µí•™)
- Google Scholar (í†µí•©)
- Semantic Scholar (AI ê°•í™”)

### 7. í˜‘ì—… í•„í„°ë§ ì—†ìŒ
**í˜„ì¬:** Content-basedë§Œ (ì‚¬ìš©ì ë¬¸ì„œ ê¸°ë°˜)

**ê°œì„ :** Hybrid ì¶”ì²œ
- ë¹„ìŠ·í•œ ì—°êµ¬ìê°€ ì½ì€ ë…¼ë¬¸
- ìì£¼ í•¨ê»˜ ì¸ìš©ë˜ëŠ” ë…¼ë¬¸
- ê°™ì€ ì£¼ì œ í´ëŸ¬ìŠ¤í„°

## ğŸ¯ ìš°ì„ ìˆœìœ„ ê°œì„  ì‚¬í•­

### Phase 1: ì¦‰ì‹œ êµ¬í˜„ ê°€ëŠ¥ (1-2ì‹œê°„)

#### A. arXiv API ì™„ì„±
```python
# core/recommendation/journal_fetcher.py

def fetch_recent_articles(self, category, max_results=50):
    response = requests.get(self.api_url, params=params, timeout=30)
    response.raise_for_status()

    root = ET.fromstring(response.content)
    namespace = {'atom': 'http://www.w3.org/2005/Atom',
                 'arxiv': 'http://arxiv.org/schemas/atom'}

    articles = []
    for entry in root.findall('atom:entry', namespace):
        article = {
            'title': entry.find('atom:title', namespace).text.strip(),
            'abstract': entry.find('atom:summary', namespace).text.strip(),
            'authors': [author.find('atom:name', namespace).text
                       for author in entry.findall('atom:author', namespace)],
            'year': int(entry.find('atom:published', namespace).text[:4]),
            'doi': entry.find('atom:id', namespace).text,
            'journal': 'arXiv'
        }
        articles.append(article)

    return articles
```

#### B. ì¶”ì²œ ìºì‹± êµ¬í˜„
```python
# core/recommendation/engine.py

def _get_cached_recommendations(self, journal_name, max_age_hours=24):
    db = self.workspace.get_database()
    cursor = db.connect().cursor()

    cutoff_time = datetime.now() - timedelta(hours=max_age_hours)

    rows = cursor.execute("""
        SELECT * FROM recommendation_cache
        WHERE journal_name = ? AND fetched_at > ?
        ORDER BY similarity_score DESC
    """, (journal_name, cutoff_time.isoformat())).fetchall()

    return [dict(row) for row in rows]

def _save_to_cache(self, journal_name, recommendations):
    db = self.workspace.get_database()

    with db.transaction() as conn:
        cursor = conn.cursor()

        for rec in recommendations:
            cursor.execute("""
                INSERT INTO recommendation_cache
                (journal_name, article_title, article_abstract, article_authors,
                 article_year, article_doi, similarity_score, reason)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (journal_name, rec.article_title, rec.article_abstract,
                  str(rec.article_authors), rec.article_year, rec.article_doi,
                  rec.similarity_score, rec.reason))
```

#### C. ì €ë„ ì¦ê²¨ì°¾ê¸° ë§¤ë‹ˆì €
```python
# core/favorite_journal_manager.py (ì‹ ê·œ ìƒì„±)

class FavoriteJournalManager:
    def __init__(self, workspace):
        self.workspace = workspace

    def add_favorite(self, journal_name, issn=None, update_frequency='weekly'):
        db = self.workspace.get_database()
        with db.transaction() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT OR REPLACE INTO favorite_journals
                (journal_name, issn, update_frequency, is_active)
                VALUES (?, ?, ?, 1)
            """, (journal_name, issn, update_frequency))

        return cursor.lastrowid

    def get_all_favorites(self):
        db = self.workspace.get_database()
        cursor = db.connect().cursor()
        rows = cursor.execute("""
            SELECT * FROM favorite_journals WHERE is_active = 1
            ORDER BY journal_name
        """).fetchall()
        return [dict(row) for row in rows]

    def update_last_fetched(self, journal_id):
        db = self.workspace.get_database()
        with db.transaction() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                UPDATE favorite_journals
                SET last_fetched = CURRENT_TIMESTAMP
                WHERE journal_id = ?
            """, (journal_id,))

    def scan_all_favorites(self, recommendation_engine):
        """ëª¨ë“  ì¦ê²¨ì°¾ê¸° ì €ë„ì—ì„œ ì¶”ì²œ ìƒì„±"""
        favorites = self.get_all_favorites()
        all_recommendations = []

        for journal in favorites:
            logger.info(f"Scanning journal: {journal['journal_name']}")

            recommendations = recommendation_engine.generate_recommendations(
                journal['journal_name'],
                days_back=30
            )

            all_recommendations.extend(recommendations)
            self.update_last_fetched(journal['journal_id'])

        return all_recommendations
```

### Phase 2: UI ê°œì„  (1-2ì‹œê°„)

#### D. ì¶”ì²œ ë‹¤ì´ì–¼ë¡œê·¸ ê°œì„ 
- ì¦ê²¨ì°¾ê¸° ì €ë„ ëª©ë¡ ì¶”ê°€
- ì—¬ëŸ¬ ì†ŒìŠ¤ ì„ íƒ (Crossref, arXiv)
- ì¶”ì²œ ê²°ê³¼ì— í”¼ë“œë°± ë²„íŠ¼
- ë…¼ë¬¸ ìƒì„¸ ë¯¸ë¦¬ë³´ê¸°

#### E. ì¶”ì²œ ì„¤ëª… ê°œì„ 
```python
def _explain_recommendation(self, article, score):
    explanation = []

    # ìœ ì‚¬ë„ ì ìˆ˜
    if score > 0.7:
        explanation.append("â­â­â­ ë§¤ìš° ë†’ì€ ê´€ë ¨ì„±")
    elif score > 0.5:
        explanation.append("â­â­ ë†’ì€ ê´€ë ¨ì„±")
    else:
        explanation.append("â­ ì¤‘ê°„ ê´€ë ¨ì„±")

    # ê³µí†µ í‚¤ì›Œë“œ
    keywords = self.vectorizer.get_common_keywords(article_text, top_k=3)
    if keywords:
        explanation.append(f"ê³µí†µ í‚¤ì›Œë“œ: {', '.join(keywords)}")

    # ìœ ì‚¬ ë¬¸ì„œ
    similar_doc = self._find_most_similar_user_doc(article)
    if similar_doc:
        explanation.append(f"'{similar_doc[:40]}...'ì™€(ê³¼) ìœ ì‚¬")

    # ì €ì ê²¹ì¹¨
    common_authors = self._find_common_authors(article)
    if common_authors:
        explanation.append(f"ê³µí†µ ì €ì: {', '.join(common_authors)}")

    return ' | '.join(explanation), keywords
```

### Phase 3: ê³ ê¸‰ ê¸°ëŠ¥ (3-5ì‹œê°„)

#### F. ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•©
- PubMed, Semantic Scholar API ì¶”ê°€
- ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì¡°ì •
- ì¤‘ë³µ ì œê±° (DOI ê¸°ë°˜)

#### G. í˜‘ì—… í•„í„°ë§
- ìœ ì‚¬ ì‚¬ìš©ì ì°¾ê¸° (íƒœê·¸, ì €ì ê¸°ë°˜)
- í•¨ê»˜ ì¸ìš©ë˜ëŠ” ë…¼ë¬¸ ì¶”ì²œ
- ë…¼ë¬¸ í´ëŸ¬ìŠ¤í„°ë§ (topic modeling)

#### H. ì‚¬ìš©ì í”¼ë“œë°± í•™ìŠµ
- í”¼ë“œë°± DB ì„¤ê³„
- ì¶”ì²œ ìŠ¤ì½”ì–´ì— í”¼ë“œë°± ë°˜ì˜
- A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### 1. ìºì‹± ì „ëµ
- ì¶”ì²œ ê²°ê³¼ ìºì‹± (24ì‹œê°„)
- ë²¡í„°í™” ê²°ê³¼ ìºì‹±
- API ì‘ë‹µ ìºì‹±

### 2. ë³‘ë ¬ ì²˜ë¦¬
- ì—¬ëŸ¬ ì €ë„ ë™ì‹œ ìŠ¤ìº”
- ë²¡í„°í™” ë³‘ë ¬ ì²˜ë¦¬

### 3. ì¸ë±ì‹±
- ì¶”ì²œ ìºì‹œì— ì¸ë±ìŠ¤ ì¶”ê°€
```sql
CREATE INDEX IF NOT EXISTS idx_cache_journal_date
ON recommendation_cache(journal_name, fetched_at DESC);

CREATE INDEX IF NOT EXISTS idx_cache_score
ON recommendation_cache(similarity_score DESC);
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- Vectorizer ì •í™•ë„
- API íŒŒì‹± ë¡œì§
- ìºì‹± ë™ì‘

### í†µí•© í…ŒìŠ¤íŠ¸
- ì „ì²´ ì¶”ì²œ íŒŒì´í”„ë¼ì¸
- ì—¬ëŸ¬ ì†ŒìŠ¤ í†µí•©
- UI ì›Œí¬í”Œë¡œìš°

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- 100+ ë¬¸ì„œ ì½”í¼ìŠ¤
- 1000+ ì¶”ì²œ í›„ë³´
- API rate limit ì²˜ë¦¬

## ğŸ¨ UX ê°œì„ 

### ì¶”ì²œ ì¹´ë“œ ë””ìì¸
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â­â­â­ ë†’ì€ ê´€ë ¨ì„± (92% ìœ ì‚¬ë„)                      â”‚
â”‚                                                     â”‚
â”‚ ğŸ“„ Title: Deep Learning for Energy Prediction      â”‚
â”‚ âœï¸ Authors: John Doe, Jane Smith                   â”‚
â”‚ ğŸ“… 2024 | ğŸ“š Energy & Environmental Science         â”‚
â”‚                                                     â”‚
â”‚ ğŸ”‘ Keywords: machine learning, energy, prediction  â”‚
â”‚ ğŸ’¡ ì´ìœ : 'ML for Buildings'ì™€(ê³¼) ìœ ì‚¬            â”‚
â”‚                                                     â”‚
â”‚ [ğŸ‘ Useful]  [ğŸ‘ Not Useful]  [ğŸ“¥ Download]  [ğŸ”— DOI] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í•„í„°ë§/ì •ë ¬
- ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ì •ë ¬
- ì—°ë„ í•„í„°
- ì €ë„ í•„í„°
- í‚¤ì›Œë“œ í•„í„°

## ğŸ“ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Immediate (ì™„ë£Œ ëª©í‘œ)
- [ ] arXiv XML íŒŒì‹± êµ¬í˜„
- [ ] ì¶”ì²œ ìºì‹± êµ¬í˜„
- [ ] FavoriteJournalManager í´ë˜ìŠ¤ ìƒì„±
- [ ] ì¶”ì²œ ì„¤ëª… ê°œì„ 

### Short-term (1ì£¼)
- [ ] ì¦ê²¨ì°¾ê¸° UI ì¶”ê°€
- [ ] í”¼ë“œë°± ë²„íŠ¼ ì¶”ê°€
- [ ] ì¶”ì²œ ë‹¤ì´ì–¼ë¡œê·¸ ê°œì„ 
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

### Long-term (1ë‹¬)
- [ ] PubMed, Semantic Scholar í†µí•©
- [ ] í˜‘ì—… í•„í„°ë§
- [ ] í”¼ë“œë°± ê¸°ë°˜ í•™ìŠµ
- [ ] ìë™ ì¶”ì²œ ì•Œë¦¼

## ğŸ”— ì°¸ê³  ìë£Œ

- [Crossref API ë¬¸ì„œ](https://api.crossref.org/)
- [arXiv API ë¬¸ì„œ](http://arxiv.org/help/api/)
- [PubMed E-utilities](https://www.ncbi.nlm.nih.gov/books/NBK25501/)
- [Semantic Scholar API](https://api.semanticscholar.org/)
- [scikit-learn TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
